{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "emopia_clean.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Adapted from EMOPIA/workspace/transformer/generate.ipynb from https://github.com/annahung31/EMOPIA"
      ],
      "metadata": {
        "id": "V7twu2-0fvei"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWYOhhASmY1F",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# install muspy\n",
        "!pip install muspy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check gpu version\n",
        "!nvcc --version"
      ],
      "metadata": {
        "id": "UioCnDFwmfkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install torch\n",
        "!pip install torch==1.7.0 torchvision==0.8.0 torchaudio==0.7.0"
      ],
      "metadata": {
        "id": "ktkeU2-TmrT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install pytorch fast transformers\n",
        "!pip install --user pytorch-fast-transformers "
      ],
      "metadata": {
        "id": "SjOE4aZmm_Zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clone EMOPIA repo\n",
        "!git clone https://github.com/annahung31/EMOPIA.git"
      ],
      "metadata": {
        "id": "So_GF2donpNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install EMOPIA dependencies\n",
        "!pip install -r EMOPIA/requirements.txt"
      ],
      "metadata": {
        "id": "yEpuZuVprPdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install midiSynth\n",
        "!pip install midiSynth"
      ],
      "metadata": {
        "id": "7m_6EbRLzunQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install fluidynth\n",
        "!sudo apt install libfluidsynth-dev"
      ],
      "metadata": {
        "id": "pHGTi9_g1tZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install ipdb\n",
        "!pip install ipdb"
      ],
      "metadata": {
        "id": "xsoF2N5H10ZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install miditoolkit\n",
        "!pip install miditoolkit"
      ],
      "metadata": {
        "id": "2vWgWZJs2KuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After running all of the above, restart runtime"
      ],
      "metadata": {
        "id": "p0BbxdFzcyIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import midiSynth\n",
        "from midiSynth.synth import MidiSynth\n",
        "midi_synth = MidiSynth()"
      ],
      "metadata": {
        "id": "WtmR4ZNmctj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "import os\n",
        "import pickle\n",
        "import torch"
      ],
      "metadata": {
        "id": "vBOVfWoa1hZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# in file directory, move utils.py and models.py from EMOPIA/workspace/transformer to directory the notebook is in\n",
        "# if you get an error with transformer, restart runtime and try again\n",
        "from utils import write_midi\n",
        "from models import TransformerModel, network_paras"
      ],
      "metadata": {
        "id": "hJ3_LUM21xvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Dictionary"
      ],
      "metadata": {
        "id": "RsrtMKw52Wrt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download dictionary\n",
        "!gdown --id 17dKUf33ZsDbHC5Z6rkQclge3ppDTVCMP\n",
        "!unzip co-representation.zip -d ../../dataset/\n",
        "!rm co-representation.zip"
      ],
      "metadata": {
        "id": "O6JrHFnU2Qth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check that loaded\n",
        "path_dictionary = '../../dataset/co-representation/dictionary.pkl'\n",
        "assert os.path.exists(path_dictionary)"
      ],
      "metadata": {
        "id": "Z7YU8ujZ2aSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# open dictionary\n",
        "dictionary = pickle.load(open(path_dictionary, 'rb'))\n",
        "event2word, word2event = dictionary"
      ],
      "metadata": {
        "id": "-XLKYdNQ2cFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# config\n",
        "n_class = []   # num of classes for each token\n",
        "for key in event2word.keys():\n",
        "    n_class.append(len(dictionary[0][key]))\n",
        "n_token = len(n_class)"
      ],
      "metadata": {
        "id": "C42HgJXE2dl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Model"
      ],
      "metadata": {
        "id": "EGtoAB0J22Wi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# once you run, it probably won't work and will give you the option to manually download\n",
        "# use that option to download manually and then upload to same directory as the notebook in file explorer\n",
        "!gdown --id 19Seq18b2JNzOamEQMG1uarKjj27HJkHu --output exp/pretrained_transformer.zip"
      ],
      "metadata": {
        "id": "3zLcHpJK21GA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unzip pretrained transforemer\n",
        "!unzip pretrained_transformer.zip -d exp/"
      ],
      "metadata": {
        "id": "BN5V5sbQ4az9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove zip file\n",
        "!rm pretrained_transformer.zip\n",
        "os.listdir('exp/pretrained_transformer')"
      ],
      "metadata": {
        "id": "wOmG7npP5LQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check thaat loaded\n",
        "path_saved_ckpt = 'exp/pretrained_transformer/loss_25_params.pt'\n",
        "assert os.path.exists(path_saved_ckpt)"
      ],
      "metadata": {
        "id": "GN7xKenj24xU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize model\n",
        "net = TransformerModel(n_class, is_training=False)\n",
        "net.cuda()\n",
        "net.eval()\n",
        "\n",
        "net.load_state_dict(torch.load(path_saved_ckpt))"
      ],
      "metadata": {
        "id": "SjW0QxoO3rlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Start Generating"
      ],
      "metadata": {
        "id": "kX-anDEe3tu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST EXAMPLE- SETUP\n",
        "emotion_tag = 4  # the target emotion class you want. It should belongs to [1,2,3,4].\n",
        "path_outfile = 'demo' # output midi file name"
      ],
      "metadata": {
        "id": "OpbXrNr_3rrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST EXAMPLE - RUN CONDITIONAL GENERATOR\n",
        "res, _ = net.inference_from_scratch(dictionary, emotion_tag, n_token=8, display=False)\n",
        "write_midi(res, path_outfile + '.mid', word2event)\n",
        "\n",
        "#midi_synth.play_midi(path_outfile + '.mid')\n",
        "midi_synth.midi2audio(path_outfile + '.mid', path_outfile + '.mp3')"
      ],
      "metadata": {
        "id": "HqEc2Mn933YB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Run on Performance Inputs\n"
      ],
      "metadata": {
        "id": "qF8apyJgcpvr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#raw valence inputs- CHANGE FILE NAME\n",
        "import numpy as np\n",
        "\n",
        "with open(\"cinderella_valence.csv\") as file_name:\n",
        "    raw_valence = np.loadtxt(file_name, delimiter=\",\")\n",
        "    \n",
        "print(raw_valence)"
      ],
      "metadata": {
        "id": "6NLJrl_C5TSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#raw arousal inputs- CHANGE FILE NAME\n",
        "with open(\"cinderella_arousal.csv\") as file_name:\n",
        "    raw_arousal = np.loadtxt(file_name, delimiter=\",\")\n",
        "\n",
        "print(raw_arousal)"
      ],
      "metadata": {
        "id": "9uRUpb5TdGuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to calculate emotion tag given valence and arousal scores\n",
        "def quartile(valence, arousal):\n",
        "    if valence <= 0 and arousal <= 0:\n",
        "        return 3\n",
        "    #low valence, high arousal = q2\n",
        "    elif valence <= 0 and arousal > 0:\n",
        "        return 2\n",
        "    #high valence, low arousal = q4\n",
        "    elif valence > 0 and arousal <= 0:\n",
        "        return 4\n",
        "    #high valence, high arousal = q1\n",
        "    elif valence > 0 and arousal > 0:\n",
        "        return 1  "
      ],
      "metadata": {
        "id": "bKcop403eKAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#make output directory\n",
        "!mkdir outputs"
      ],
      "metadata": {
        "id": "Y5Mz2MIXe05r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate averages and tag\n",
        "avg_valence = sum(raw_valence) / len(raw_valence)\n",
        "avg_arousal = sum(raw_arousal) / len(raw_arousal)\n",
        "print(avg_valence, avg_arousal)\n",
        "emotion_tag = quartile(avg_valence, avg_arousal)\n",
        "print(emotion_tag)\n",
        "\n",
        "# run conditional generator with avg\n",
        "path_name = 'outputs/avg_output_q' + str(emotion_tag)\n",
        "res, _ = net.inference_from_scratch(dictionary, emotion_tag, n_token=8, display=False)\n",
        "write_midi(res, path_name + '.mid', word2event)\n",
        "midi_synth.play_midi(path_name + '.mid')\n",
        "midi_synth.midi2audio(path_name + '.mid', path_name + '.mp3')"
      ],
      "metadata": {
        "id": "igCp_VHo-VbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statistics import median\n",
        "\n",
        "# calculate medians and tag\n",
        "median_valence = median(raw_valence)\n",
        "median_arousal = median(raw_arousal)\n",
        "print(median_valence, median_arousal)\n",
        "emotion_tag = quartile(median_valence, median_arousal)\n",
        "print(emotion_tag)\n",
        "\n",
        "# run conditional generator with median\n",
        "path_name = 'outputs/median_output_q' + str(emotion_tag)\n",
        "res, _ = net.inference_from_scratch(dictionary, emotion_tag, n_token=8, display=False)\n",
        "write_midi(res, path_name + '.mid', word2event)\n",
        "midi_synth.play_midi(path_name + '.mid')\n",
        "midi_synth.midi2audio(path_name + '.mid', path_name + '.mp3')"
      ],
      "metadata": {
        "id": "AmVcWvCw-WJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate maximums and tag\n",
        "max_valence = max(raw_valence)\n",
        "max_arousal = max(raw_arousal)\n",
        "print(max_valence, max_arousal)\n",
        "emotion_tag = quartile(max_amelie_valence, max_amelie_arousal)\n",
        "print(emotion_tag)\n",
        "\n",
        "# run conditional generator with maximum\n",
        "path_name = 'outputs/maximum_output_q' + str(emotion_tag)\n",
        "res, _ = net.inference_from_scratch(dictionary, emotion_tag, n_token=8, display=False)\n",
        "write_midi(res, path_name + '.mid', word2event)\n",
        "midi_synth.play_midi(path_name + '.mid')\n",
        "midi_synth.midi2audio(path_name + '.mid', path_name + '.mp3')"
      ],
      "metadata": {
        "id": "V_pxFDrp_6ro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate minimums and tag\n",
        "min_arousal = min(raw_arousal)\n",
        "min_valence = min(raw_valence)\n",
        "print(min_valence, min_arousal)\n",
        "emotion_tag = quartile(min_amelie_valence, min_amelie_arousal)\n",
        "print(emotion_tag)\n",
        "\n",
        "# run conditional generator with minimum\n",
        "path_name = 'outputs/minimum_output_q' + str(emotion_tag)\n",
        "res, _ = net.inference_from_scratch(dictionary, emotion_tag, n_token=8, display=False)\n",
        "write_midi(res, path_name + '.mid', word2event)\n",
        "midi_synth.play_midi(path_name + '.mid')\n",
        "midi_synth.midi2audio(path_name + '.mid', path_name + '.mp3')"
      ],
      "metadata": {
        "id": "Z6VXb-mQAMNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#run conditional generator for all pairs\n",
        "emopia_inputs = []\n",
        "for i in range(len(raw_valence)):\n",
        "    emotion_tag = quartile(raw_valence[i], raw_arousal[i])\n",
        "    path_name = 'outputs/output_' + str(i) + '_q' + str(emotion_tag)\n",
        "    print('i=' + str(i) + ' q'+ str(emotion_tag))\n",
        "    res, _ = net.inference_from_scratch(dictionary, emotion_tag, n_token=8, display=False)\n",
        "    write_midi(res, path_name + '.mid', word2event)\n",
        "    midi_synth.play_midi(path_name + '.mid')\n",
        "    midi_synth.midi2audio(path_name + '.mid', path_name + '.mp3')\n"
      ],
      "metadata": {
        "id": "DBz9opAYE-Rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# zip output directory so you can easily download from file explorer\n",
        "!zip -r outputs.zip outputs"
      ],
      "metadata": {
        "id": "P2dTw4VoIT8M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
