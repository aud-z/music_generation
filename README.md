# E-Musical: Art & Machine Learning Project 3 (Spring 2022)

This is the repo for E-Musical: a process for music generation using emotions from videos (through text and image analysis). 

Image-to-emotion generation uses EMONET, with code adapted from Github repository for the paper _"Estimation of continuous valence and arousal levels from faces in naturalistic conditions"_, by authors Antoine Toisoul, Jean Kossaifi, Adrian Bulat, Georgios Tzimiropoulos and Maja Pantic, published in Nature Machine Intelligence, January 2021 [[1]](#Citation). 

Emotion-to-music generation uses EMOPIA, with code adapted from the Github repository for the paper _"EMOPIA: A Multi-Modal Pop Piano Dataset For Emotion Recognition and Emotion-based Music Generation"_, by authors Anna Hung, Andrea Jansson, Seung Heon Doh, and Joann Ching. [[1]](#Citation).


[1] "Estimation of continuous valence and arousal levels from faces in naturalistic conditions", Antoine Toisoul, Jean Kossaifi, Adrian Bulat, Georgios Tzimiropoulos and Maja Pantic, published in Nature Machine Intelligence, January 2021

[2] "EMOPIA: A Multi-Modal Pop Piano Dataset For Emotion Recognition and Emotion-based Music Generation", Anna Hung, Andrea Jansson, Seung Heon Doh, and Joann Ching, arXiv preprint arXiv:2108.01374, August 2021
