# E-Musical: Art & Machine Learning Project 3 (Spring 2022)

This is the repo for E-Musical: a process for music generation using emotions from videos (through text and image analysis). 

Image-to-emotion generation uses EMONET, with code adapted from Github repository for the paper _"Estimation of continuous valence and arousal levels from faces in naturalistic conditions"_, by authors Antoine Toisoul, Jean Kossaifi, Adrian Bulat, Georgios Tzimiropoulos and Maja Pantic, published in Nature Machine Intelligence, January 2021 [[1]](#Citation). 

Emotion-to-music generation uses EMOPIA, with code adapted from the Github repository for the paper _"EMOPIA: A Multi-Modal Pop Piano Dataset For Emotion Recognition and Emotion-based Music Generation"_, by authors Hsiao-Tzu Hung, Joann Ching, Seungheon Doh, Nabin Kim, Juhan Nam, Yi-Hsuan Yang. [[2]](#Citation).


[1] "Estimation of continuous valence and arousal levels from faces in naturalistic conditions", Antoine Toisoul, Jean Kossaifi, Adrian Bulat, Georgios Tzimiropoulos and Maja Pantic, published in Nature Machine Intelligence, January 2021

[2] "EMOPIA: A Multi-Modal Pop Piano Dataset For Emotion Recognition and Emotion-based Music Generation", Hsiao-Tzu Hung, Joann Ching, Seungheon Doh, Nabin Kim, Juhan Nam, and Yi-Hsuan Yang, arXiv preprint arXiv:2108.01374, August 2021
